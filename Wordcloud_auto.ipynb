{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 社畜丼ワードクラウド"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://qiita.com/kenmatsu4/items/9b6ac74f831443d29074"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wordcloud_auto.py\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from PIL import Image\n",
    "import MeCab as mc\n",
    "\n",
    "from mastodon import Mastodon\n",
    "\n",
    "#Mastodon.create_app(\"D's toot trends App\", api_base_url = \"https://mstdn-workers.com\", to_file = \"my_clientcred_workers.txt\")\n",
    "#mastodon = Mastodon(client_id=\"my_clientcred_workers.txt\",api_base_url = \"https://mstdn-workers.com\")\n",
    "#mastodon.log_in(\"mail address\", \"passwd\",to_file = \"my_usercred_workers.txt\")\n",
    "mastodon = Mastodon(\n",
    "    client_id=\"my_clientcred_workers.txt\",\n",
    "    access_token=\"my_usercred_workers.txt\",\n",
    "    api_base_url = \"https://mstdn-workers.com\"\n",
    ")\n",
    "\n",
    "def mecab_analysis(text):\n",
    "    mecab_flags = [\n",
    "        '-Ochasen',\n",
    "        '-d /usr/lib/mecab/dic/mecab-ipadic-neologd/',\n",
    "        '-u username.dic',\n",
    "    ]\n",
    "    t = mc.Tagger(' '.join(mecab_flags))\n",
    "    enc_text = text.strip() # MeCabに渡した文字列は必ず変数に入れておく https://shogo82148.github.io/blog/2012/12/15/mecab-python/\n",
    "    t.parse('') # UnicodeDecodeError対策 http://taka-say.hateblo.jp/entry/2015/06/24/183748 \n",
    "    node = t.parseToNode(enc_text)\n",
    "    output = []\n",
    "    while(node):\n",
    "        if node.surface != \"\":  # ヘッダとフッタを除外\n",
    "            word_type = node.feature.split(\",\")[0]\n",
    "            if word_type in [\"形容詞\", \"名詞\", \"副詞\"]:\n",
    "                output.append(node.surface)\n",
    "        node = node.next\n",
    "        if node is None:\n",
    "            break\n",
    "    return output\n",
    "\n",
    "def create_wordcloud(text, background_image='background'):\n",
    "\n",
    "    # 環境に合わせてフォントのパスを指定する。\n",
    "    #fpath = \"/System/Library/Fonts/HelveticaNeue-UltraLight.otf\"\n",
    "    #fpath = \"/Library/Fonts/ヒラギノ角ゴ Pro W3.otf\"\n",
    "    fpath = \"/usr/share/fonts/opentype/noto/NotoSansCJK-Medium.ttc\"\n",
    "\n",
    "    # ストップワードの設定\n",
    "    stop_words = [ u'てる', u'いる', u'なる', u'れる', u'する', u'ある', u'こと', u'これ', u'さん', u'して', \\\n",
    "             u'くれる', u'やる', u'くださる', u'そう', u'せる', u'した',  u'思う',  \\\n",
    "             u'それ', u'ここ', u'ちゃん', u'くん', u'', u'て',u'に',u'を',u'は',u'の', u'が', u'と', u'た', u'し', u'で', \\\n",
    "             u'ない', u'も', u'な', u'い', u'か', u'ので', u'よう', u'']\n",
    "    \n",
    "    img_array = np.array(Image.open(background_image))\n",
    "    image_colors = ImageColorGenerator(img_array)\n",
    "    \n",
    "    wordcloud = WordCloud(regexp=r\"\\w[\\w']*\",\n",
    "                          background_color=\"white\",\n",
    "                          font_path=fpath,\n",
    "                          mask=img_array,\n",
    "                          stopwords=set(stop_words),\n",
    "#                          max_font_size=55, \n",
    "                         ).generate(text)\n",
    "\n",
    "    plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\")\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.savefig(\"/tmp/wordcloud.png\", \n",
    "                dpi=200,\n",
    "                bbox_inches = 'tight',\n",
    "                pad_inches = 0)\n",
    "\n",
    "def str2datetime(s):\n",
    "    from datetime import datetime\n",
    "    import dateutil.parser\n",
    "    from pytz import timezone\n",
    "    return dateutil.parser.parse(s).astimezone(timezone('Asia/Tokyo'))\n",
    "\n",
    "def get_ranged_toots(time_begin, time_end):\n",
    "    tl_ = []\n",
    "    from time import sleep\n",
    "    max_id = None\n",
    "    running = True\n",
    "    while running:\n",
    "        tl = mastodon.timeline(\n",
    "            timeline='local',\n",
    "            max_id=max_id,\n",
    "            since_id=None,\n",
    "            limit=40)\n",
    "        max_id = tl[-1]['id']\n",
    "        for toot in tl:\n",
    "            created_at = str2datetime(toot['created_at'])\n",
    "            if created_at < time_begin:\n",
    "                running = False\n",
    "                break\n",
    "            if created_at >= time_end:\n",
    "                continue\n",
    "            tl_.append(toot2dict(toot))\n",
    "        if not running:\n",
    "            break\n",
    "        sleep(1.5)\n",
    "    df_ranged = pd.DataFrame.from_records(tl_)\n",
    "    return df_ranged\n",
    "\n",
    "def toot2dict(toot):\n",
    "    t = {}\n",
    "    t['id'] = toot['id']\n",
    "    t['created_at'] = toot['created_at']\n",
    "    t['username'] = toot['account']['username']\n",
    "    t['toot'] = toot['content']\n",
    "    if toot['spoiler_text'] != '':\n",
    "        t['toot'] = toot['spoiler_text']\n",
    "    return t\n",
    "\n",
    "def filter_df(df):\n",
    "    filter_suffix = [\n",
    "        '_info', '_infom', '_information', '_material',\n",
    "    ]\n",
    "    return df[\n",
    "        ~df['username'].map(\n",
    "            lambda s:\n",
    "                any([s.lower().endswith(sfx)\n",
    "                     for sfx in filter_suffix])\n",
    "        )\n",
    "    ]\n",
    "\n",
    "def toot_convert(toots):\n",
    "    import re, html\n",
    "    return toots.map(\n",
    "        lambda s: re.sub('<[^>]*>', '', s)\n",
    "    ).map(\n",
    "        lambda s: re.sub(r'https?://[^ ]+', \"\", s)\n",
    "    ).map(\n",
    "        lambda s: html.unescape(s)\n",
    "    )\n",
    "\n",
    "from datetime import datetime, timedelta, date\n",
    "import pytz\n",
    "\n",
    "jst = pytz.timezone('Asia/Tokyo')\n",
    "now = datetime.now(jst)\n",
    "today = now.date()\n",
    "today = jst.localize(datetime(today.year, today.month, today.day))\n",
    "\n",
    "hour_end = now.timetuple().tm_hour\n",
    "time_range = [hour_end-1, hour_end]\n",
    "\n",
    "def time_pair(today, time_begin, time_end):\n",
    "    return [today+timedelta(hours=h) for h in [time_begin, time_end]]\n",
    "[time_begin, time_end] = time_pair(today, *time_range)\n",
    "\n",
    "def get_toot_str(today, time_begin, time_end):\n",
    "    time_str = [t.strftime('%H:%M') for t in [time_begin, time_end]]\n",
    "    return f\"{time_begin.date()} {'-'.join(time_str)}\\n#社畜丼トレンド\"\n",
    "\n",
    "time_range = time_pair(today, *time_range)\n",
    "toot_str = get_toot_str(today, *time_range)\n",
    "\n",
    "df_ranged = get_ranged_toots(*time_range)\n",
    "# 全トゥートを結合して形態素解析に流し込んで単語に分割する\n",
    "wordlist = mecab_analysis(' '.join(toot_convert(filter_df(df_ranged)['toot']).iloc[::-1].tolist()))\n",
    "\n",
    "import re\n",
    "#一文字ひらがな、カタカナを削除\n",
    "wordlist = [w for w in wordlist if not re.match('^[あ-んーア-ンーｱ-ﾝｰ]$', w)]\n",
    "#返ってきたリストを結合してワードクラウドにする\n",
    "create_wordcloud(' '.join(wordlist))\n",
    "\n",
    "if (\"post\" in sys.argv):\n",
    "    media_file = mastodon.media_post('/tmp/wordcloud.png')\n",
    "    mastodon.status_post(status=toot_str, media_ids=[media_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run wordcloud_auto.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('/tmp/wordcloud.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
